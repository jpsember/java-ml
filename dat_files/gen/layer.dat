// Defines a layer within a convolutional neural network

enum gen.LayerType;

// Model-specific output layers may override features such as dropout and batch_normalization.
// Special consideration is given to the input layer as well.
//

fields {

  LayerType type;

  // For CONV layers; if zero, uses neural_network.kernel_width
  //  ** TODO: make optional to use default
  //
  int kernel_width;

  // For CONV, the number of filters in this layer (the kernel's depth extent).
  // For FC or OUTPUT, the number of values in the output 'fibre' (1 x 1 x n)
  //
  int filters;

  // For CONV layers: if true, employs a stride of 2 to reduce the output volume by a factor of 2,
  // to achieve approximately the same effect as a separate max_pool layer;
  // alternatively, have an explicit maxpool layer
  //
  bool pool;

  // For LEAKY_RELU layers; if zero, uses neural_network.alpha
  //  ** TODO: make optional to use default
  //
  float alpha;

  // For CONV, MAXPOOL layers; if absent, uses [1,1] or network default respectively
  //
  ? IPoint stride;

  // If nonzero, and network is for training, a dropout layer is added to the end of the layer to randomly drop out
  // elements with this probability.
  //
  // Values < 0 are treated as 0, and are resistent to the pojo 'apply defaults' algorithm.
  //  ** DEPRECATED: make this OPTIONAL so zero can be explicitly added
  //
  float dropout;

  // ------------------------------------------------------------------
  // Generated by analyzer
  //

  int num_weights;

  Vol input_volume;

  Vol output_volume;
}
