fields {

  File target_dir_train = "train_data";
  File target_dir_checkpoint = "checkpoints";

  // Streaming service attempts to maintain this many
  // sets in the directory; this should be one more than the number that
  // the training code needs
  //
  int max_train_sets = 3;

  // Maximum number of times a train set is reused before being discarded
  //
  int recycle = 3;

  int max_checkpoints = 5;

  // For certain project types (e.g. classification)
  //
  int target_accuracy = 95;

  // Stops if training loss drops to this value
  //
  float target_loss;

  // If nonzero, training stops if epoch reaches this value
  //
  int target_epoch;

  // https://arxiv.org/abs/1804.07612
  //
  // "The presented results confirm that using small batch sizes achieves
  //  the best training stability and generalization performance, for a given
  //  computational cost, across a wide range of experiments. In all cases
  //  the best results have been obtained with batch sizes m = 32 or smaller,
  //  often as small as m = 2 or m = 4."
  //
  int batch_size = 32;


  // Have pytorch add extra checks for things like NaN values
  //
  bool detect_anomalies;

  // Maximum number of tensors to display logging information for
  //
  int max_log_count = 20;

  // Apply gradient normalization if it gets too large (https://androidkt.com/how-to-apply-gradient-clipping-in-pytorch/)
  //
  bool with_gradient_norm;

  // Disable batch normalization (to see effect on training)?
  //
  bool disable_batch_norm;
}


